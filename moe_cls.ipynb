{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ee60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moe.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertLayer,\n",
    "    BertOutput,\n",
    "    BertLMPredictionHead,\n",
    ")\n",
    "\n",
    "\n",
    "class MoEFFN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int = 4,\n",
    "        expert_size: Optional[int] = None,\n",
    "        k: int = 2,\n",
    "        dropout_prob: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.expert_size = expert_size or hidden_size * 4\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, self.expert_size),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_prob),\n",
    "                nn.Linear(self.expert_size, hidden_size),\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.gate = nn.Linear(hidden_size, num_experts, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        batch_size, seq_len, hidden_dim = hidden_states.shape\n",
    "        assert hidden_dim == self.hidden_size\n",
    "\n",
    "        x = hidden_states.view(-1, hidden_dim)  # [N, H]\n",
    "        gate_logits = self.gate(x)  # [N, E]\n",
    "        top_k_logits, top_k_indices = torch.topk(gate_logits, self.k, dim=1)  # [N, k]\n",
    "        top_k_weights = torch.softmax(top_k_logits, dim=1)  # [N, k]\n",
    "\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        for i in range(self.num_experts):\n",
    "            expert_mask = (top_k_indices == i)  # [N, k]\n",
    "            if expert_mask.any():\n",
    "                token_indices = expert_mask.nonzero(as_tuple=True)[0]  # [M]\n",
    "                pos_in_topk = expert_mask.nonzero(as_tuple=True)[1]    # [M]\n",
    "\n",
    "                expert_inputs = x[token_indices]  # [M, H]\n",
    "                expert_weights = top_k_weights[token_indices, pos_in_topk]  # [M]\n",
    "                expert_out = self.experts[i](expert_inputs)  # [M, H]\n",
    "                weighted_out = expert_out * expert_weights.unsqueeze(-1)  # [M, H]\n",
    "\n",
    "                final_output.index_add_(0, token_indices, weighted_out)\n",
    "\n",
    "        return final_output.view(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertLayer\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertLayerWithMoE(BertLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π FFN\n",
    "        del self.intermediate\n",
    "        del self.output\n",
    "\n",
    "        self.moe_ffn = MoEFFN(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_experts=getattr(config, \"num_experts\", 4),\n",
    "            expert_size=config.intermediate_size,  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ MoE\n",
    "            k=getattr(config, \"moe_k\", 2),\n",
    "            dropout_prob=config.hidden_dropout_prob,\n",
    "        )\n",
    "\n",
    "        # –í–º–µ—Å—Ç–æ BertOutput ‚Äî —Å–æ–∑–¥–∞—ë–º —Å–≤–æ–π –ø—Ä–æ—Å—Ç–æ–π LayerNorm + Dropout\n",
    "        self.moe_output_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.moe_output_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self_attn_output = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = self_attn_output[0]\n",
    "\n",
    "        moe_output = self.moe_ffn(attn_output)  # [B, L, hidden_size]\n",
    "\n",
    "        # Residual + Dropout + LayerNorm (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º BERT)\n",
    "        moe_output = self.moe_output_dropout(moe_output)\n",
    "        layer_output = self.moe_output_layer_norm(attn_output + moe_output)\n",
    "\n",
    "        outputs = (layer_output,) + self_attn_output[1:]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertMoEForMaskedLM(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        # –°–æ–∑–¥–∞—ë–º BERT –∏ –∑–∞–º–µ–Ω—è–µ–º —Å–ª–æ–∏ –Ω–∞ MoE\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            layer.__class__ = BertLayerWithMoE\n",
    "            layer.__init__(config)\n",
    "\n",
    "        self.cls = BertLMPredictionHead(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # –ü–µ—Ä–µ–¥–∞—ë–º –¢–û–õ–¨–ö–û –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤ BertModel\n",
    "        bert_kwargs = {\n",
    "            k: v for k, v in {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "                \"position_ids\": position_ids,\n",
    "                \"head_mask\": head_mask,\n",
    "                \"inputs_embeds\": inputs_embeds,\n",
    "                \"output_attentions\": output_attentions,\n",
    "                \"output_hidden_states\": output_hidden_states,\n",
    "                \"return_dict\": return_dict,\n",
    "            }.items() if v is not None\n",
    "        }\n",
    "\n",
    "        outputs = self.bert(**bert_kwargs)\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                prediction_scores.view(-1, self.config.vocab_size),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": prediction_scores,\n",
    "            \"hidden_states\": outputs.hidden_states,\n",
    "            \"attentions\": outputs.attentions,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainConfig:\n",
    "    model_name = \"your-moe-bert\"\n",
    "    dataset_name = \"wikimedia/wikipedia\"\n",
    "    dataset_config = \"20231101.en\"\n",
    "    text_column = \"text\"\n",
    "    tokenizer = \"bert-base-uncased\"\n",
    "    output_dir = \".\"\n",
    "    seq_len = 128\n",
    "    batch_size = 32\n",
    "\n",
    "    masking_prob = 0.15\n",
    "\n",
    "    lr = 5e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 1000\n",
    "    max_steps = 10_000\n",
    "\n",
    "    save_steps = 5_000\n",
    "    logging_steps = 100\n",
    "    eval_steps = 2000\n",
    "\n",
    "    # BERT / MoE –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "    bert_hidden_size = 256\n",
    "    bert_intermediate_size = 1024\n",
    "    bert_num_hidden_layers = 4\n",
    "    bert_num_attention_heads = 4\n",
    "    num_experts = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9664f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import BertConfig\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    cfg = PretrainConfig()\n",
    "\n",
    "    # --- –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ---\n",
    "    model_config = BertConfig(\n",
    "        vocab_size=30522,\n",
    "        hidden_size=cfg.bert_hidden_size,\n",
    "        num_hidden_layers=cfg.bert_num_hidden_layers,\n",
    "        num_attention_heads=cfg.bert_num_attention_heads,\n",
    "        intermediate_size=cfg.bert_intermediate_size,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        pad_token_id=0,\n",
    "        num_experts=cfg.num_experts,\n",
    "        moe_k=2,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer)\n",
    "    model = BertMoEForMaskedLM(model_config)\n",
    "\n",
    "    print(f\"Model initialized with {cfg.num_experts} experts.\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # --- –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê –í STREAMING –†–ï–ñ–ò–ú–ï ---\n",
    "    print(\"Loading dataset in streaming mode...\")\n",
    "    dataset = load_dataset(\n",
    "        cfg.dataset_name,\n",
    "        cfg.dataset_config,\n",
    "        split=\"train\",\n",
    "        streaming=True  # üî• –∫–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ!\n",
    "    )\n",
    "\n",
    "    # --- –§–£–ù–ö–¶–ò–Ø –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò (–±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –ª–µ–Ω–∏–≤–æ) ---\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[cfg.text_column],\n",
    "            truncation=True,\n",
    "            padding=False,  # collator —Å–∞–º —Å–¥–µ–ª–∞–µ—Ç padding –¥–æ batch max\n",
    "            max_length=cfg.seq_len,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —É–¥–∞–ª—è–µ–º –í–°–ï –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "    original_columns = dataset.column_names  # ['id', 'text', 'url'] ‚Äî –¥–ª—è wikipedia\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=original_columns,  # ‚Üê —É–¥–∞–ª—è–µ–º –í–°–Å, –∫—Ä–æ–º–µ output tokenizer'–∞\n",
    "    )\n",
    "\n",
    "    # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ –≤ streaming!) ---\n",
    "    def filter_short(example):\n",
    "        return len(example[\"input_ids\"]) >= cfg.seq_len // 2\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.filter(filter_short)\n",
    "\n",
    "    # --- Data collator ---\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=True,\n",
    "        mlm_probability=cfg.masking_prob,\n",
    "    )\n",
    "\n",
    "    # --- Training args ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        max_steps=cfg.max_steps,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        warmup_steps=cfg.warmup_steps,\n",
    "        logging_steps=cfg.logging_steps,\n",
    "        save_steps=cfg.save_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=False,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=2,  # –º–æ–∂–Ω–æ 0‚Äì4, –Ω–æ –≤ streaming –ª—É—á—à–µ 0‚Äì2\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",\n",
    "        # ‚ö†Ô∏è –í–ê–ñ–ù–û: –æ—Ç–∫–ª—é—á–∞–µ–º shuffle –¥–ª—è streaming (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—É—Ñ–µ—Ä)\n",
    "        dataloader_drop_last=True,\n",
    "        save_safetensors=False\n",
    "    )\n",
    "\n",
    "    # --- –°–æ–∑–¥–∞—ë–º Trainer ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,  # ‚Üê streaming dataset!\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # --- –û–±—É—á–µ–Ω–∏–µ ---\n",
    "    print(\"Starting pretraining (streaming)...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---\n",
    "    final_dir = os.path.join(cfg.output_dir, \"final_model\")\n",
    "    trainer.save_model(final_dir)\n",
    "    tokenizer.save_pretrained(final_dir)\n",
    "    print(f\"Model saved to {final_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36436a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_dir = \"/content/final_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "model = BertMoEForMaskedLM.from_pretrained(model_dir)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "import torch\n",
    "\n",
    "text = \"Paris is the [MASK] of France.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs[\"logits\"]\n",
    "\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "mask_positions = (inputs[\"input_ids\"] == mask_token_id).nonzero(as_tuple=False)\n",
    "\n",
    "batch_idx, mask_pos = mask_positions[0].tolist()\n",
    "\n",
    "mask_logits = logits[batch_idx, mask_pos, :]\n",
    "top_k = torch.topk(mask_logits, k=10)\n",
    "top_ids = top_k.indices.tolist()\n",
    "top_scores = top_k.values.tolist()\n",
    "\n",
    "print(\"Input:\", text)\n",
    "print(\"Top predictions for [MASK]:\")\n",
    "for token_id, score in zip(top_ids, top_scores):\n",
    "    token = tokenizer.decode([token_id])\n",
    "    print(f\"{token!r}  logit={score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moe_multilabel_model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "\n",
    "class BertMoEForMultiLabelClassification(BertMoEForMaskedLM):\n",
    "    \"\"\"\n",
    "    –ú—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–≤–æ–µ–π MLM-–º–æ–¥–µ–ª–∏ BertMoEForMaskedLM.\n",
    "    - bert + moe –±–µ—Ä—É—Ç—Å—è –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "    - MLM-–≥–æ–ª–æ–≤–∞ (self.cls) –æ—Å—Ç–∞—ë—Ç—Å—è, –Ω–æ –≤ —ç—Ç–æ–º forward –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\n",
    "    - –ø–æ–≤–µ—Ä—Ö –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è multilabel-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞ —Å BCEWithLogitsLoss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        # –ø–æ–ª–µ–∑–Ω–æ —è–≤–Ω–æ –ø–æ–º–µ—Ç–∏—Ç—å —Ç–∏–ø –∑–∞–¥–∞—á–∏\n",
    "        self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        # post_init –≤ —Ä–æ–¥–∏—Ç–µ–ª–µ —É–∂–µ –±—ã–ª –≤—ã–∑–≤–∞–Ω –≤ super().__init__\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,  # [batch, num_labels] —Å 0/1\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # 1) –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ BERT+MoE (–∫–∞–∫ –≤ MLM-–∫–ª–∞—Å—Å–µ, –Ω–æ –±–µ–∑ MLM-–≥–æ–ª–æ–≤—ã)\n",
    "        bert_kwargs = {\n",
    "            k: v for k, v in {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "                \"position_ids\": position_ids,\n",
    "                \"head_mask\": head_mask,\n",
    "                \"inputs_embeds\": inputs_embeds,\n",
    "                \"output_attentions\": output_attentions,\n",
    "                \"output_hidden_states\": output_hidden_states,\n",
    "                \"return_dict\": return_dict,\n",
    "            }.items() if v is not None\n",
    "        }\n",
    "\n",
    "        outputs = self.bert(**bert_kwargs)\n",
    "        sequence_output = outputs.last_hidden_state  # [B, L, H]\n",
    "\n",
    "        # 2) –ò—Å–ø–æ–ª—å–∑—É–µ–º [CLS] —Ç–æ–∫–µ–Ω\n",
    "        cls_output = sequence_output[:, 0, :]        # [B, H]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)         # [B, num_labels]\n",
    "\n",
    "        # 3) –õ–æ—Å—Å –¥–ª—è multilabel\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # labels: int/float {0,1}, —Ä–∞–∑–º–µ—Ä [B, num_labels]\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = [\n",
    "    \"Computer Science\",\n",
    "    \"Physics\",\n",
    "    \"Mathematics\",\n",
    "    \"Statistics\",\n",
    "    \"Quantitative Biology\",\n",
    "    \"Quantitative Finance\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel_config.py\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiLabelConfig:\n",
    "    # –ü—É—Ç—å –∫ —Ç–≤–æ–µ–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π MLM-MoE –º–æ–¥–µ–ª–∏ (BertMoEForMaskedLM)\n",
    "    pretrained_mlm_path: str = \"/content/final_model\"  # ‚Üê –ó–ê–ú–ï–ù–ò\n",
    "\n",
    "    # –ü—É—Ç—å –∫ Kaggle csv\n",
    "    train_csv: str = \"/content/data/train.csv\"\n",
    "    test_csv: str = \"/content/data/train.csv\"\n",
    "\n",
    "    # –ö–æ–ª–æ–Ω–∫–∏\n",
    "    title_column: str = \"TITLE\"\n",
    "    abstract_column: str = \"ABSTRACT\"\n",
    "    label_columns: List[str] = field(default_factory=lambda: [\n",
    "        \"Computer Science\",\n",
    "        \"Physics\",\n",
    "        \"Mathematics\",\n",
    "        \"Statistics\",\n",
    "        \"Quantitative Biology\",\n",
    "        \"Quantitative Finance\",\n",
    "    ])\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer_name: str = \"bert-base-uncased\"  # –∏–ª–∏ —Ç–æ—Ç, —á—Ç–æ —Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –¥–ª—è MLM\n",
    "\n",
    "    max_length: int = 256\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 16\n",
    "    num_train_epochs: int = 3\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    logging_steps: int = 50\n",
    "    save_steps: int = 500\n",
    "    output_dir: str = \"./moe_multilabel\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97497578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_multilabel.py (–ª–æ–≥–∏–∫–∞ –≤–Ω—É—Ç—Ä–∏ train-—Å–∫—Ä–∏–ø—Ç–∞, —Å–º. –Ω–∏–∂–µ)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "cfg = MultiLabelConfig()\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name, use_fast=True)\n",
    "\n",
    "raw_dataset = load_dataset(\"csv\", data_files={\"train\": cfg.train_csv})[\"train\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # –¢–∏—Ç–ª + –∞–±—Å—Ç—Ä–∞–∫—Ç –ø–æ–¥–∞—ë–º –∫–∞–∫ –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "    tokenized = tokenizer(\n",
    "        examples[cfg.title_column],\n",
    "        examples[cfg.abstract_column],\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(examples[cfg.title_column])):\n",
    "        labels.append([\n",
    "            examples[col][i] for col in cfg.label_columns\n",
    "        ])\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "processed_dataset = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names,  # —É–¥–∞–ª—è–µ–º ID, TITLE, ABSTRACT, label-–∫–æ–ª–æ–Ω–∫–∏\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832cfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_multilabel_moe.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "def build_compute_metrics_fn(threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ Trainer –∫–∞–∫ compute_metrics.\n",
    "    –î–ª—è multilabel:\n",
    "    - logits -> sigmoid -> >= threshold -> 0/1\n",
    "    - —Å—á–∏—Ç–∞–µ—Ç macro/micro precision/recall/F1\n",
    "    \"\"\"\n",
    "    def compute_metrics(eval_pred: EvalPrediction):\n",
    "        logits, labels = eval_pred\n",
    "        # labels: shape [N, num_labels]\n",
    "        # logits: shape [N, num_labels]\n",
    "\n",
    "        # 1) —Å–∏–≥–º–æ–∏–¥–∞\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "        # 2) –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è\n",
    "        y_pred = (probs >= threshold).astype(int)\n",
    "        y_true = labels.astype(int)\n",
    "\n",
    "        # 3) —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "        # micro: —Å—É–º–º–∞—Ä–Ω–æ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º\n",
    "        precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"micro\", zero_division=0\n",
    "        )\n",
    "        # macro: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"precision_micro\": precision_micro,\n",
    "            \"recall_micro\": recall_micro,\n",
    "            \"f1_micro\": f1_micro,\n",
    "            \"precision_macro\": precision_macro,\n",
    "            \"recall_macro\": recall_macro,\n",
    "            \"f1_macro\": f1_macro,\n",
    "        }\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    cfg = MultiLabelConfig()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name, use_fast=True)\n",
    "\n",
    "    raw_dataset = load_dataset(\"csv\", data_files={\"train\": cfg.train_csv})[\"train\"]\n",
    "\n",
    "    dataset_splits = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = dataset_splits[\"train\"]\n",
    "    eval_dataset = dataset_splits[\"test\"]\n",
    "\n",
    "    LABEL_COLUMNS = cfg.label_columns\n",
    "    print(\"–ö–æ–ª–æ–Ω–∫–∏:\", raw_dataset.column_names)\n",
    "    print(\"Label-–∫–æ–ª–æ–Ω–∫–∏:\", LABEL_COLUMNS)\n",
    "\n",
    "    base_config = AutoConfig.from_pretrained(cfg.pretrained_mlm_path)\n",
    "    base_config.num_labels = len(LABEL_COLUMNS)\n",
    "    base_config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "    model = BertMoEForMultiLabelClassification.from_pretrained(\n",
    "        cfg.pretrained_mlm_path,\n",
    "        config=base_config,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: TITLE + ABSTRACT –∫–∞–∫ –ø–∞—Ä–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        tokenized = tokenizer(\n",
    "            examples[cfg.title_column],\n",
    "            examples[cfg.abstract_column],\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_length,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "        for i in range(len(examples[cfg.title_column])):\n",
    "            labels.append([\n",
    "                examples[col][i] for col in LABEL_COLUMNS\n",
    "            ])\n",
    "        tokenized[\"labels\"] = labels\n",
    "        return tokenized\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_dataset.column_names,\n",
    "    )\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_dataset.column_names,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=cfg.num_train_epochs,\n",
    "        per_device_train_batch_size=cfg.train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.eval_batch_size,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        logging_steps=cfg.logging_steps,\n",
    "        save_steps=cfg.save_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=cfg.save_steps,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        save_safetensors=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=build_compute_metrics_fn(threshold=0.5)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    final_dir = os.path.join(cfg.output_dir, \"final_model\")\n",
    "    trainer.save_model(final_dir)\n",
    "    tokenizer.save_pretrained(final_dir)\n",
    "    print(f\"Model saved to {final_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ac67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_multilabel_from_test_csv.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "def load_model_for_inference(model_dir: str, cfg: MultiLabelConfig):\n",
    "    config = AutoConfig.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "    model = BertMoEForMultiLabelClassification.from_pretrained(model_dir, config=config)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def predict_on_test_csv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    cfg: MultiLabelConfig,\n",
    "    threshold: float = 0.5,\n",
    "    batch_size: int = 32,\n",
    "    output_path: str = \"sample_submission.csv\",\n",
    "):\n",
    "    # 1) –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π CSV\n",
    "    test_df = pd.read_csv(cfg.test_csv)  # ‚Üê –≤ –∫–æ–Ω—Ñ–∏–≥–µ —É–∂–µ –µ—Å—Ç—å test_dir\n",
    "    print(\"–¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç:\", test_df.shape)\n",
    "    print(\"–ö–æ–ª–æ–Ω–∫–∏ –≤ —Ç–µ—Å—Ç–µ:\", list(test_df.columns))\n",
    "\n",
    "    # –û–∂–∏–¥–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã ID, TITLE, ABSTRACT\n",
    "    id_col = \"ID\"\n",
    "\n",
    "    # 2) –ü—Ä–æ–≥–æ–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏\n",
    "    all_preds = []\n",
    "\n",
    "    num_labels = len(cfg.label_columns)\n",
    "    n = len(test_df)\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        batch = test_df.iloc[start:end]\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            batch[cfg.title_column].tolist(),\n",
    "            batch[cfg.abstract_column].tolist(),\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            logits = outputs[\"logits\"]  # [B, num_labels]\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        preds = (probs >= threshold).astype(int)  # 0/1\n",
    "        all_preds.append(preds)\n",
    "\n",
    "    all_preds = np.vstack(all_preds)  # [N, num_labels]\n",
    "    assert all_preds.shape == (len(test_df), num_labels)\n",
    "\n",
    "    # 3) –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    submission_df = pd.DataFrame(\n",
    "        all_preds,\n",
    "        columns=cfg.label_columns,\n",
    "    )\n",
    "    submission_df.insert(0, id_col, test_df[id_col].values)\n",
    "\n",
    "    # 4) –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ sample_submission.csv\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ {output_path}\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = MultiLabelConfig()\n",
    "\n",
    "    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –≥–¥–µ –ª–µ–∂–∏—Ç –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "    model_dir = \"/content/moe_multilabel/checkpoint-3500\"  # –∫–∞–∫ –≤ train-—Å–∫—Ä–∏–ø—Ç–µ\n",
    "\n",
    "    model, tokenizer, device = load_model_for_inference(model_dir, cfg)\n",
    "\n",
    "    predict_on_test_csv(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        cfg=cfg,\n",
    "        threshold=0.5,\n",
    "        batch_size=32,\n",
    "        output_path=\"sample_submission.csv\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
