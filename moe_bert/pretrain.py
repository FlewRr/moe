# train.py (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ‚Äî –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞)
import os
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    set_seed,
)
from config import PretrainConfig
from moe import BertMoEForMaskedLM
from transformers import BertConfig


def main():
    set_seed(42)
    cfg = PretrainConfig()

    # --- –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ---
    model_config = BertConfig(
        vocab_size=30522,
        hidden_size=cfg.bert_hidden_size,
        num_hidden_layers=cfg.bert_num_hidden_layers,
        num_attention_heads=cfg.bert_num_attention_heads,
        intermediate_size=cfg.bert_intermediate_size,
        hidden_act="gelu",
        hidden_dropout_prob=0.1,
        attention_probs_dropout_prob=0.1,
        max_position_embeddings=512,
        type_vocab_size=2,
        pad_token_id=0,
        num_experts=cfg.num_experts,
        moe_k=2,
    )

    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer)
    model = BertMoEForMaskedLM(model_config)

    print(f"Model initialized with {cfg.num_experts} experts.")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")

    # --- –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê –í STREAMING –†–ï–ñ–ò–ú–ï ---
    print("Loading dataset in streaming mode...")
    dataset = load_dataset(
        cfg.dataset_name,
        cfg.dataset_config,
        split="train",
        streaming=True  # üî• –∫–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ!
    )

    # --- –§–£–ù–ö–¶–ò–Ø –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò (–±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –ª–µ–Ω–∏–≤–æ) ---
    def tokenize_function(examples):
        return tokenizer(
            examples[cfg.text_column],
            truncation=True,
            padding=False,  # collator —Å–∞–º —Å–¥–µ–ª–∞–µ—Ç padding –¥–æ batch max
            max_length=cfg.seq_len,
            return_special_tokens_mask=True,
        )

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —É–¥–∞–ª—è–µ–º –í–°–ï –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    original_columns = dataset.column_names  # ['id', 'text', 'url'] ‚Äî –¥–ª—è wikipedia
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=original_columns,  # ‚Üê —É–¥–∞–ª—è–µ–º –í–°–Å, –∫—Ä–æ–º–µ output tokenizer'–∞
    )

    # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ –≤ streaming!) ---
    def filter_short(example):
        return len(example["input_ids"]) >= cfg.seq_len // 2

    tokenized_dataset = tokenized_dataset.filter(filter_short)

    # --- Data collator ---
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=True,
        mlm_probability=cfg.masking_prob,
    )

    # --- Training args ---
    training_args = TrainingArguments(
        output_dir=cfg.output_dir,
        overwrite_output_dir=True,
        max_steps=cfg.max_steps,
        per_device_train_batch_size=cfg.batch_size,
        gradient_accumulation_steps=1,
        learning_rate=cfg.lr,
        weight_decay=cfg.weight_decay,
        warmup_steps=cfg.warmup_steps,
        logging_steps=cfg.logging_steps,
        save_steps=cfg.save_steps,
        save_strategy="steps",
        load_best_model_at_end=False,
        fp16=True,
        dataloader_num_workers=2,  # –º–æ–∂–Ω–æ 0‚Äì4, –Ω–æ –≤ streaming –ª—É—á—à–µ 0‚Äì2
        remove_unused_columns=False,
        report_to="none",
        # ‚ö†Ô∏è –í–ê–ñ–ù–û: –æ—Ç–∫–ª—é—á–∞–µ–º shuffle –¥–ª—è streaming (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—É—Ñ–µ—Ä)
        dataloader_drop_last=True,
    )

    # --- –°–æ–∑–¥–∞—ë–º Trainer ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,  # ‚Üê streaming dataset!
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # --- –û–±—É—á–µ–Ω–∏–µ ---
    print("Starting pretraining (streaming)...")
    trainer.train()

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
    final_dir = os.path.join(cfg.output_dir, "final_model")
    trainer.save_model(final_dir)
    tokenizer.save_pretrained(final_dir)
    print(f"Model saved to {final_dir}")


if __name__ == "__main__":
    main()